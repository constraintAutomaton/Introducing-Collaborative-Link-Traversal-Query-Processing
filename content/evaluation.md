## Evaluation
{:#evaluation}
To evaluate this work, I plan to build a prototype and test it against an existing RDF social media benchmark [](cite:cites Angles2020TheLS, Angles2014TheLD, Spasic2016).
Additionally, I will use the prototype to test a use case involving self-publication of academic papers.

For evaluation metrics, I will take inspiration from the authors discussed in the literature review [](#litterature_review) and consider the following measurements: **Number of engine**, **Result completeness**, **Ability to access isolated documents**, **Number of requests**, **Overlapping of the search space exploration of the Query engine**, **Query execution time**, **Rate of production of results**, **Delay between the first result and the start of the execution**, **Cache miss and rate of use of the cache**
